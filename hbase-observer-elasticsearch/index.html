<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> 使用Observer实现HBase到Elasticsearch的数据同步 · ScienJus's Blog</title><meta name="description" content="使用Observer实现HBase到Elasticsearch的数据同步 - ScienJus"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="http://www.scienjus.com/atom.xml" title="ScienJus's Blog"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/favicon.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="http://weibo.com/ScienJus" target="_blank" class="nav-list-link">WEIBO</a></li><li class="nav-list-item"><a href="https://github.com/ScienJus" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">RSS</a></li><li class="nav-list-item"><a href="/about/" target="_self" class="nav-list-link">ABOUT</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">使用Observer实现HBase到Elasticsearch的数据同步</h1><div class="post-info">Jun 4, 2016</div><div class="post-content"><h3 id="多数据源的数据同步"><a href="#多数据源的数据同步" class="headerlink" title="多数据源的数据同步"></a>多数据源的数据同步</h3><p> 多个数据源中的数据同步问题，无非就三种解决方式：</p>
<ol>
<li>客户端双写，分别将数据写入两个数据源（同步、异步）</li>
<li>主数据源在收到数据后推给辅数据源（同步、异步）</li>
<li><p>辅数据源从主数据源中拉取数据（轮训、监听，全量、增量）</p>
<p>具体到 HBase 同步到 Elasticsearch 时，后两种方式具体对应的方案就是 HBase 的 Observer 和 Elasticsearch 的 River，这两种方式都可以使开发者在数据源中嵌入自己的业务逻辑，并且依托于集群可以轻松地保证高可用。</p>
<p>但是非常遗憾的是，要使用 River 高效的同步数据，必须要有一种拉取增量数据的方式，而在 HBase 中这并没有很好的方法实现，所以本文将会介绍使用 Observer 的方法。</p>
<p>题外话：Elasticsearch 的 MySQL River 有两种实现：<code>elasticsearch-river-jdbc</code>和<code>elasticsearch-river-mysql</code>。前者简单的通过 SQL 查询数据同步到 Elasticsearch，所以必须要在表中定义更新时间的字段才能完成增量更新，而且它无法得知哪些数据删除掉了，除非增加并使用逻辑删除字段。而后者则通过 MySQL 的主从复制机制，读取 Binlog 完成增量数据的同步，要更加方便和实用很多。</p>
</li>
</ol>
<h3 id="什么是-Observer"><a href="#什么是-Observer" class="headerlink" title="什么是 Observer"></a>什么是 Observer</h3><p>HBase 0.92 版本引入了协处理器（Coprocessor），可以使开发者将自己的代码嵌入到 HBase 中，其中协处理器分为两大块，一个是终端（Endpoint），另一个是本文将要介绍的观察者（Observer）。</p>
<p>Observer 有些类似于 MySQL 中的触发器（Trigger），它可以为 HBase 中的操作添加钩子，并在事件发生后实现自己的的业务逻辑。</p>
<p>Observer 主要分为三种：</p>
<ul>
<li>RegionObserver：增删改查相关，例如 Get、Put、Delete、Scan 等</li>
<li>WALObserver：WAL 操作相关</li>
<li><p>MasterObserver：DDL-类型相关，例如创建、删除、修改数据表等</p>
<p>数据同步将会使用 RegionObserver 监听 Put 和 Delete 事件。</p>
</li>
</ul>
<h3 id="如何实现自己的-Observer"><a href="#如何实现自己的-Observer" class="headerlink" title="如何实现自己的 Observer"></a>如何实现自己的 Observer</h3><p> 每一个 Observer 都是一个 Jar 包。首先需要引入<code>hbase-server</code>包，并实现如<code>BaseRegionObserver</code>等 HBase 提供的相关接口，重写需要监听对应事件的方法。</p>
<p> 实现数据同步功能可以重写<code>postPut</code>和<code>putDelete</code>方法监听 Put 和 Delete 事件。</p>
<p> 下面就是一个最简单的例子，在这两个方法中分别得到表名和 RowKey，然后输出到 HBase 默认的日志中：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">public class SimpleObserver extends BaseRegionObserver &#123;</span><br><span class="line"></span><br><span class="line">    private static final Log logger = LogFactory.getLog (SimpleObserver.class);</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public void postPut (ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, Put put, WALEdit edit, Durability durability) throws IOException &#123;</span><br><span class="line">        // 拿到表名</span><br><span class="line">        String table = e.getEnvironment ().getRegion ().getRegionInfo ().getTable ().getNameAsString ();</span><br><span class="line">        // 拿到 row key</span><br><span class="line">        String rowKey = new String (delete.getRow ());</span><br><span class="line">        logger.info (&quot;a put event! table: &quot; + table + &quot;, row key: &quot; + rowKey);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public void postDelete (ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, Delete delete, WALEdit edit, Durability durability) throws IOException &#123;</span><br><span class="line">        // 拿到表名</span><br><span class="line">        String table = e.getEnvironment ().getRegion ().getRegionInfo ().getTable ().getNameAsString ();</span><br><span class="line">        // 拿到 row key</span><br><span class="line">        String rowKey = new String (delete.getRow ());</span><br><span class="line">        logger.info (&quot;a delete event! table: &quot; + table + &quot;, row key: &quot; + rowKey);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p> 之后将项目打包，上传到 HDFS 中：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -mkdir /observers</span><br><span class="line">hdfs dfs -put simple-observer.jar /observers</span><br></pre></td></tr></table></figure>
<p> 使用 HBase Shell 创建一个表，将这个 Observer 挂到该表中：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">create &apos;test_observer&apos;</span><br><span class="line"></span><br><span class="line">disable &apos;test_observer&apos;</span><br><span class="line"></span><br><span class="line">alter ‘test_observer&apos;, METHOD =&gt; &apos;table_att&apos;, &apos;coprocessor&apos; =&gt; &apos;hdfs:///observers/simple-observer.jar|com.scienjus.observer.SimpleObserver|&apos;</span><br><span class="line"></span><br><span class="line">enable &apos;test_observer&apos;</span><br></pre></td></tr></table></figure>
<p><code>coprocessor</code>的值是一个字符串，由以下几个部分组成：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jar 地址（如果在配置文件中定义了 CLASS_PATH 可以不填）|类名（包含包路径）|优先级|自定义属性</span><br></pre></td></tr></table></figure>
<p> 此时通过<code>describe</code>可以看到这个表已经挂上了观察者：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">describe &apos;test_observer&apos;</span><br><span class="line"></span><br><span class="line">Table test_observer is ENABLED</span><br><span class="line"></span><br><span class="line">test_observer, &#123;TABLE_ATTRIBUTES =&gt; &#123;coprocessor$1 =&gt; &apos;hdfs:///observers/simple-observer.jar|com.scienjus.observer.SimpleObserver|&apos;&#125;</span><br><span class="line">COLUMN FAMILIES DESCRIPTION</span><br><span class="line">&#123;NAME =&gt; &apos;info&apos;, DATA_BLOCK_ENCODING =&gt; &apos;NONE&apos;, BLOOMFILTER =&gt; &apos;ROW&apos;, REPLICATION_SCOPE =&gt; &apos;0&apos;, VERSIO</span><br><span class="line">NS =&gt; &apos;1&apos;, COMPRESSION =&gt; &apos;NONE&apos;, MIN_VERSIONS =&gt; &apos;0&apos;, TTL =&gt; &apos;FOREVER&apos;, KEEP_DELETED_CELLS =&gt; &apos;FALSE&apos;</span><br><span class="line">, BLOCKSIZE =&gt; &apos;65536&apos;, IN_MEMORY =&gt; &apos;false&apos;, BLOCKCACHE =&gt; &apos;true&apos;&#125;</span><br><span class="line">1 row (s) in 0.2600 seconds</span><br></pre></td></tr></table></figure>
<p> 向这个表中进行 Put 和 Delete 操作，就可以看到对应的日志了。</p>
<h3 id="如何同步数据到-Elasticsearch"><a href="#如何同步数据到-Elasticsearch" class="headerlink" title="如何同步数据到 Elasticsearch"></a>如何同步数据到 Elasticsearch</h3><p>Elasticsearch 官方的 Java 客户端提供了一个名为<code>BulkProcessor</code>的接口，这个接口可以轻易的实现一个批量发送请求的缓冲池。</p>
<p> 下面这段代码创建了一个缓冲池，它会定期批量发送堆积的请求，触发条件为：</p>
<ul>
<li>每 2 秒触发一次</li>
<li>当堆积的请求数量达到 1000 个时，触发一次</li>
<li>当堆积的请求达到 100mb 时，触发一次</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">processor = BulkProcessor.builder (client, new BulkProcessor.Listener () &#123;</span><br><span class="line">    @Override</span><br><span class="line">    public void beforeBulk (long executionId, BulkRequest request) &#123;</span><br><span class="line">        logger.info (&quot;before bulk !!!&quot;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public void afterBulk (long executionId, BulkRequest request, BulkResponse response) &#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public void afterBulk (long executionId, BulkRequest request, Throwable failure) &#123;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;)</span><br><span class="line">        .setBulkActions (1000)</span><br><span class="line">        .setBulkSize (new ByteSizeValue (100, ByteSizeUnit.MB))</span><br><span class="line">        .setFlushInterval (TimeValue.timeValueSeconds (2))</span><br><span class="line">        .setConcurrentRequests (5)</span><br><span class="line">        .build ();</span><br></pre></td></tr></table></figure>
<p> 同时它还提供了一个监听器，可以在发送请求前、发送请求后、发送请求出现异常时监听到对应事件并进行处理。可以在其中处理失败情况，例如重发或是记录日志。</p>
<p> 将 Observer 和 BulkProcessor 结合起来，只需要在 postPut 时将文档转为 JSON 生成 Upsert 请求加入缓冲池，在 postDelete 时将 RowKey 作为 id 生成删除请求加入缓冲池即可，例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">@Override</span><br><span class="line">public void postPut (ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, Put put, WALEdit edit, Durability durability) throws IOException &#123;</span><br><span class="line">    try &#123;</span><br><span class="line">        // 拿到表名</span><br><span class="line">        String table = e.getEnvironment ().getRegion ().getRegionInfo ().getTable ().getNameAsString ();</span><br><span class="line">        // 拿到 id</span><br><span class="line">        String id = new String (put.getRow ());</span><br><span class="line">        logger.info (&quot;a put! table: &quot; + table + &quot;, key: &quot; + id);</span><br><span class="line">        // 拿到文档内容</span><br><span class="line">        Map&lt;String, String&gt; doc = new HashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">        for (List&lt;Cell&gt; cells : put.getFamilyCellMap ().values ()) &#123;</span><br><span class="line">            for (Cell cell : cells) &#123;</span><br><span class="line">                doc.put (new String (CellUtil.cloneQualifier (cell)), new String (CellUtil.cloneValue (cell)));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        processor.add (new UpdateRequest ()</span><br><span class="line">                .index (index)</span><br><span class="line">                .type (type)</span><br><span class="line">                .id (id)</span><br><span class="line">                .doc (doc)</span><br><span class="line">                .docAsUpsert (true)</span><br><span class="line">        );</span><br><span class="line">    &#125; catch (RuntimeException ex) &#123;</span><br><span class="line">        // TODO 记录运行异常</span><br><span class="line">        logger.info (&quot;error!&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public void postDelete (ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, Delete delete, WALEdit edit, Durability durability) throws IOException &#123;</span><br><span class="line">    try &#123;</span><br><span class="line">        // 拿到表名</span><br><span class="line">        String table = e.getEnvironment ().getRegion ().getRegionInfo ().getTable ().getNameAsString ();</span><br><span class="line">        // 拿到 id</span><br><span class="line">        String id = new String (delete.getRow ());</span><br><span class="line">        logger.info (&quot;a delete! table: &quot; + table + &quot;, key: &quot; + id);</span><br><span class="line">        processor.add (new DeleteRequest ()</span><br><span class="line">                .index (index)</span><br><span class="line">                .type (type)</span><br><span class="line">                .id (id)</span><br><span class="line">        );</span><br><span class="line">    &#125; catch (RuntimeException ex) &#123;</span><br><span class="line">        // TODO 记录运行异常</span><br><span class="line">        logger.info (&quot;error!&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p> 最后别忘了监听<code>stop</code>事件，将缓冲池和客户端都关闭：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">@Override</span><br><span class="line">public void stop (CoprocessorEnvironment e) throws IOException &#123;</span><br><span class="line">    processor.close ();</span><br><span class="line">    client.close ();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</div></article></div></main><footer><div class="paginator"><a href="/elasticsearch-in-action-3/" class="prev">PREV</a><a href="/elasticsearch-in-action-2/" class="next">NEXT</a></div><div id="disqus_thread"></div><script>var disqus_shortname = 'scienjus';
var disqus_identifier = 'hbase-observer-elasticsearch/';
var disqus_title = '使用Observer实现HBase到Elasticsearch的数据同步';
var disqus_url = 'http://www.scienjus.com/hbase-observer-elasticsearch/';
(function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();</script><script id="dsq-count-scr" src="//scienjus.disqus.com/count.js" async></script><div class="copyright"><p>© 2015 - 2021 <a href="http://www.scienjus.com">ScienJus</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;e=o.createElement(i);r=o.getElementsByTagName(i)[0];e.src='//www.google-analytics.com/analytics.js';r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));ga('create',"UA-168066892-1",'auto');ga('send','pageview');</script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?47cf199e59a26235dadf455975b8b67f";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();</script></body></html>